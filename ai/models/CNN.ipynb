{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sleep Stage Scoring (on raw EEG signal)\n",
    "\n",
    "What is different from [this version](https://github.com/baroquerock/sleep-stages-scoring):\n",
    "1. Pytorch framework\n",
    "2. Multichannel input\n",
    "3. Different architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "New architecture accomodates two channels:\n",
    "\n",
    "<img src=\"img/arch2.png\" width=\"800\" height=\"1000\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import copy\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch import optim, nn\n",
    "from torchvision import transforms, datasets, models\n",
    "\n",
    "from collections import OrderedDict\n",
    "\n",
    "from npzloader import DataLoader as DataLoader_x\n",
    "import torch.utils.data as utils\n",
    "\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.metrics import (confusion_matrix, classification_report, cohen_kappa_score)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "moB1UfGjD7vW"
   },
   "outputs": [],
   "source": [
    "# loading fpz_cz and pz_oz channels\n",
    "loader = DataLoader_x(\"../data/eeg_fpz_cz\")\n",
    "data_fp, labels = loader.load_data(verbose=0)\n",
    "data_fp = {''.join(k.partition('fpz_cz')[1:]) : v for k, v in data_fp.items()} \n",
    "\n",
    "loader = DataLoader_x(\"../data/eeg_pz_oz\")\n",
    "data_pz, _ = loader.load_data(verbose=0)\n",
    "data_pz = {''.join(k.partition('pz_oz')[1:]) : v for k, v in data_pz.items()} \n",
    "\n",
    "all_labels = np.hstack(labels.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sSFEK863D7ve"
   },
   "outputs": [],
   "source": [
    "# int-to-label map\n",
    "class_dict = {0: \"W\", \n",
    "              1: \"N1\",\n",
    "              2: \"N2\",\n",
    "              3: \"N3\",\n",
    "              4: \"REM\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PbmyneTYD7wv"
   },
   "source": [
    "### TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uegNojzFD7w2"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Function to randomly select subjects for partial cross-validation and prepare the data for training.\n",
    "Train and test sets should be independent.\n",
    "A patient with is SC413 is excluded from any test set because this patient has only one recording.\n",
    "\"\"\"\n",
    "def get_cv_data(data, labels, n_folds=5, ch='fpz_cz'):\n",
    "    \n",
    "    random.seed(29)\n",
    "    test_key = []\n",
    "    keys = list(labels.keys())\n",
    "    keys = list(set([x[:5] for x in keys if '413' not in x]))\n",
    "    for i in range(len(keys)):\n",
    "        if \"419\" in keys[i]:\n",
    "            test_key = [keys[i]]\n",
    "    random.shuffle(keys)\n",
    "    print(test_key)\n",
    "    val_keys = keys[:n_folds]\n",
    " \n",
    "    cv_data = []\n",
    "    test_key = [test_key[0] + '1E0',test_key[0] + '2E0']\n",
    "    \n",
    "    for fold in val_keys:\n",
    "        fold_keys = [fold+'1E0', fold+'2E0']\n",
    "        \n",
    "        val_labels = [labels[key] for key in fold_keys]\n",
    "        val_labels = np.hstack(val_labels)\n",
    "        \n",
    "        keys_not_training = fold_keys + test_key\n",
    "        \n",
    "        train_labels = [labels[key] for key in labels.keys() if key not in keys_not_training]\n",
    "        train_labels = np.hstack(train_labels)\n",
    "       \n",
    "        test_labels=[labels[key] for key in test_key]\n",
    "        test_labels= np.hstack(test_labels)\n",
    "        \n",
    "        if ch:\n",
    "            val_data = [data['{}/{}'.format(ch,key)] for key in fold_keys]\n",
    "            train_data = [data['{}/{}'.format(ch,key)] for key in labels.keys() if key not in keys_not_training]   \n",
    "            test_data = [data['{}/{}'.format(ch,key)] for key in test_key]\n",
    "        else:\n",
    "            val_data = [data[key] for key in fold_keys]\n",
    "            train_data = [data[key] for key in labels.keys() if key not in keys_not_training]\n",
    "            test_data = [data[key] for key in labels.keys() if key in test_key]\n",
    "           \n",
    "        train_data = np.vstack(train_data)\n",
    "        val_data = np.vstack(val_data)\n",
    "        test_data = np.vstack(test_data)\n",
    "    \n",
    "        cv_data.append((train_data, train_labels, val_data, val_labels, test_data, test_labels))\n",
    "    \n",
    "    return cv_data \n",
    "\n",
    "\"\"\"\n",
    "Function to slide on array with a moving window of size n\n",
    "\"\"\"\n",
    "\n",
    "def _slide(data, labels, n=3):\n",
    "    \n",
    "    #zero-indexing\n",
    "    data = [data[i:i+n] for i, _ in enumerate(data[n:])]\n",
    "    data = [np.vstack(x) for x in data]\n",
    "    data = np.array(data)\n",
    "    \n",
    "    labels = labels[n:]\n",
    "    \n",
    "    assert data.shape[0] == labels.shape[0]\n",
    "    \n",
    "    return data, labels\n",
    "\n",
    "\"\"\"\n",
    "Function to do batch processing with _slide()\n",
    "\"\"\"\n",
    "\n",
    "def batch_slide(data_dict, labels_dict, n=3):\n",
    "    \n",
    "    keys = [(x,y) for x,y in zip(data_dict, labels_dict)]\n",
    "    data = [_slide(data_dict[x], labels_dict[y], n=n) for x,y in keys]\n",
    "    \n",
    "    data_keys, label_keys = zip(*keys)\n",
    "    data, labels = zip(*data)\n",
    "    \n",
    "    data_dict = {k:v for k,v in zip(data_keys, data)}\n",
    "    data_dict = OrderedDict(sorted(data_dict.items()))\n",
    "    \n",
    "    labels_dict = {k:v for k,v in zip(label_keys, labels)}\n",
    "    labels_dict = OrderedDict(sorted(labels_dict.items()))\n",
    "        \n",
    "    return data_dict, labels_dict \n",
    "  \n",
    "  \n",
    "\"\"\"\n",
    "Function to merge channels into single recording.\n",
    "axis = 0: merge channels along the first dimension - (3000,1) and (3000,1) --> (6000,1)\n",
    "axis = 0: merge channels along the second dimension - (3000,1) and (3000,1) --> (3000,2)\n",
    "\"\"\"  \n",
    "    \n",
    "def merge_channels(channels, axis=0):\n",
    "    \n",
    "    # number of records should be the same in all channels\n",
    "    assert len(set([len(r) for r in channels])) == 1\n",
    "    \n",
    "    keys = [ch.keys() for ch in channels]\n",
    "    \n",
    "    merged = {}\n",
    "    \n",
    "    for records in zip(*keys):\n",
    "        \n",
    "        # assert that all recordings are for the same patient\n",
    "        assert len(set([r.split('/')[1] for r in records])) == 1\n",
    "        vals = [ch[r] for r, ch in zip(records, channels)]\n",
    "        if axis:\n",
    "            stacked = np.dstack(vals)\n",
    "        else:\n",
    "            stacked = np.hstack(vals)\n",
    "        patient = records[0].split('/')[1]\n",
    "        merged[patient] = stacked\n",
    "        \n",
    "    merged = OrderedDict(sorted(merged.items()))\n",
    "    return merged\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xb01HsHBoraY"
   },
   "outputs": [],
   "source": [
    "data_all = merge_channels([data_fp, data_pz], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IEsk57FID7xH"
   },
   "outputs": [],
   "source": [
    "# getting cross-valivation data\n",
    "# note: n_folds is not a number of folds in traditional sense\n",
    "# the model is trained on 19 patients and validated on 1,\n",
    "# so n_folds is a number of patients for validation\n",
    "# to perform full cross-validation n_folds should be equal to 19 \n",
    "# (not 20, because for SC413 patient there is only one record)\n",
    "cv_data = get_cv_data(data_all, labels, ch=None, n_folds=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Otf2uGzLD7yi"
   },
   "outputs": [],
   "source": [
    "def small_conv(fs): \n",
    "    return nn.Sequential(\n",
    "            nn.Conv1d(1, 64, kernel_size=fs//2, stride=fs//16, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.MaxPool1d(kernel_size=8, stride=8),\n",
    "            \n",
    "            nn.Conv1d(64, 64, kernel_size=8, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(64, 64, kernel_size=8, padding=2),\n",
    "            nn.ReLU(), \n",
    "            nn.Conv1d(64, 64, kernel_size=8, padding=2),\n",
    "            nn.ReLU())\n",
    "\n",
    "def big_conv(fs): \n",
    "    return nn.Sequential(\n",
    "            nn.Conv1d(1, 64, kernel_size=fs*4, stride=fs//2, padding=2),\n",
    "            nn.ReLU(), \n",
    "            nn.Dropout(0.3),\n",
    "            nn.MaxPool1d(kernel_size=4, stride=4),\n",
    "            \n",
    "            nn.Conv1d(64, 64, kernel_size=6, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(64, 64, kernel_size=6, padding=2),\n",
    "            nn.ReLU(), \n",
    "            nn.Conv1d(64, 64, kernel_size=6, padding=2),\n",
    "            nn.ReLU())\n",
    "\n",
    "\n",
    "class ConvNet(nn.Module):\n",
    "  \n",
    "    def __init__(self, n_cnn_dense=256, fs=100, num_classes=5):\n",
    "      \n",
    "        super(ConvNet, self).__init__()\n",
    "        \n",
    "        self.layer1_fz = small_conv(fs)        \n",
    "        self.layer2_fz = big_conv(fs)\n",
    "\n",
    "        self.layer1_pz = small_conv(fs)        \n",
    "        self.layer2_pz = big_conv(fs)\n",
    "        \n",
    "        \n",
    "        self.fc1 = nn.Sequential(\n",
    "            nn.Linear(124, n_cnn_dense),\n",
    "            nn.ReLU(),            \n",
    "            nn.MaxPool1d(kernel_size=4, stride=4))\n",
    "        \n",
    "        self.fc2 = nn.Sequential(\n",
    "            nn.Linear(4096, num_classes),\n",
    "            nn.LogSoftmax(dim=1))\n",
    "        \n",
    "                \n",
    "        \n",
    "    def forward(self, channels):\n",
    "      \n",
    "        ch1 = channels[:, 0, :].unsqueeze(1)\n",
    "        ch2 = channels[:, 1, :].unsqueeze(1)\n",
    "        \n",
    "        out1_1 = self.layer1_fz(ch1)\n",
    "        out2_1 = self.layer2_fz(ch1)\n",
    "        \n",
    "        out1_2 = self.layer1_pz(ch2)\n",
    "        out2_2 = self.layer2_pz(ch2)        \n",
    "        \n",
    "        out = torch.cat((out1_1, out2_1, out1_2, out2_2), dim=2)\n",
    "        out = self.fc1(out)\n",
    "        s = out.size()[0]\n",
    "        out = out.view(s, -1)\n",
    "        out = self.fc2(out)\n",
    "       \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zTBFZaDKne0p"
   },
   "outputs": [],
   "source": [
    "model = ConvNet()\n",
    "train_data, train_labels, val_data, val_labels, test_data, test_labels = cv_data[3]\n",
    "\n",
    "train = utils.TensorDataset(torch.from_numpy(np.swapaxes(train_data,1,2)), torch.from_numpy(train_labels).long())\n",
    "train_loader = utils.DataLoader(train, batch_size=64, shuffle=True)\n",
    "\n",
    "valid = utils.TensorDataset(torch.from_numpy(np.swapaxes(val_data,1,2)), torch.from_numpy(val_labels).long())\n",
    "valid_loader = utils.DataLoader(valid, batch_size=64, shuffle=True)\n",
    "\n",
    "test = utils.TensorDataset(torch.from_numpy(np.swapaxes(test_data,1,2)), torch.from_numpy(test_labels).long())\n",
    "test_loader = utils.DataLoader(test, batch_size=64, shuffle=True)\n",
    "                           \n",
    "dataloaders = {'train': train_loader, 'valid': valid_loader, \"test\":test_loader}\n",
    "dataset_sizes = {'train': len(train), 'valid': len(valid), \"test\": len(test)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "40FA0Mt-qQt8"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PJyvQdmkN_aX"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Xizp4eJJ5kYy"
   },
   "outputs": [],
   "source": [
    "def train_model(model, criteria, optimizer, scheduler, num_epochs=25, device='cuda'):\n",
    "    model.to(device)\n",
    "    since = time.time()\n",
    "    epoch_loss_array=[] \n",
    "    accuracy_array=[]\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch + 1, num_epochs))\n",
    "        print('-' * 10)\n",
    "\n",
    "        for phase in ['train', 'valid',\"test\"]:\n",
    "            if phase == 'train':\n",
    "                scheduler.step()\n",
    "                model.train()  \n",
    "            else:\n",
    "                model.eval()   \n",
    "                \n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "   \n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    loss = criteria(outputs, labels)\n",
    "\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            epoch_loss_array.append(epoch_loss)\n",
    "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
    "            accuracy_array.append(epoch_acc)\n",
    "            \n",
    "\n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n",
    "                phase, epoch_loss, epoch_acc))\n",
    "\n",
    "            if phase == 'valid' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model, epoch_loss_array, accuracy_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PMXmk4v1CARM"
   },
   "outputs": [],
   "source": [
    "criteria = nn.NLLLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.005)\n",
    "sched = optim.lr_scheduler.StepLR(optimizer, step_size=4, gamma=0.1)\n",
    "eps=15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1806
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 368141,
     "status": "ok",
     "timestamp": 1546889587450,
     "user": {
      "displayName": "Tatiana Gaponova",
      "photoUrl": "https://lh5.googleusercontent.com/-gUJ79WnHrqY/AAAAAAAAAAI/AAAAAAAAQis/zrKck2fIipY/s64/photo.jpg",
      "userId": "17758316458603777674"
     },
     "user_tz": -60
    },
    "id": "Ns_SuBUhCAgP",
    "outputId": "cd0bfbcd-e4c5-49b6-cbae-644fa8493fb2"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using {}!\".format(device))\n",
    "epoch_loss_array=[] \n",
    "accuracy_array=[]\n",
    "model_ft,epoch_loss_array,accuracy_array= train_model(model, criteria, optimizer, sched, eps, device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_array_train=[]\n",
    "loss_array_valid=[]\n",
    "loss_array_test=[]\n",
    "\n",
    "\n",
    "\n",
    "for i in range(0, len(epoch_loss_array), 3):\n",
    "    loss_array_train.append(epoch_loss_array[i])\n",
    "    \n",
    "for i in range(1, len(epoch_loss_array), 3):\n",
    "    loss_array_valid.append(epoch_loss_array[i])\n",
    "\n",
    "for i in range(2, len(epoch_loss_array), 3):\n",
    "    loss_array_test.append(epoch_loss_array[i])\n",
    "plt.plot(loss_array_train)\n",
    "plt.xlabel(\"Époque\")\n",
    "plt.ylabel(\"Perte\")\n",
    "plt.title(\"Graphique du résultat de la fonction de perte en fonction de l'époque pour l'ensemble d'entrainement\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(loss_array_valid)\n",
    "plt.xlabel(\"Époque\")\n",
    "plt.ylabel(\"Perte\")\n",
    "plt.title(\"Graphique du résultat de la fonction de perte en fonction de l'époque pour l'ensemble de validation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(loss_array_test)\n",
    "plt.xlabel(\"Époque\")\n",
    "plt.ylabel(\"Perte\")\n",
    "plt.title(\"Graphique du résultat de la fonction de perte en fonction de l'époque pour l'ensemble de test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_array_train=[]\n",
    "accuracy_array_valid=[]\n",
    "accuracy_array_test=[]\n",
    "\n",
    "for i in range(0, len(accuracy_array), 3):\n",
    "    accuracy_array_train.append(accuracy_array[i])\n",
    "    \n",
    "for i in range(1,len(accuracy_array), 3):\n",
    "    accuracy_array_valid.append(accuracy_array[i])\n",
    "\n",
    "for i in range(2, len(accuracy_array), 3):\n",
    "    accuracy_array_test.append(accuracy_array[i])\n",
    "plt.plot(accuracy_array_train)\n",
    "plt.xlabel(\"Époque\")\n",
    "plt.ylabel(\"Précision\")\n",
    "plt.title(\"Graphique du résultat de la précision du modèle en fonction de l'époque pour l'ensemble d'entrainement\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.plot(accuracy_array_valid)\n",
    "plt.xlabel(\"Époque\")\n",
    "plt.ylabel(\"Précision\")\n",
    "plt.title(\"Graphique du résultat de la précision du modèle en fonction de l'époque pour l'ensemble de validation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(accuracy_array_test)\n",
    "plt.xlabel(\"Époque\")\n",
    "plt.ylabel(\"Précision\")\n",
    "plt.title(\"Graphique du résultat de la précision du modèle en fonction de l'époque pour l'ensemble de test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_keys = [key for key in labels if 'SC419' in key]\n",
    "\n",
    "for night_key in y_test_keys:\n",
    "    y_test_current = labels[night_key]\n",
    "    curent_eeg_data = np.swapaxes(data_all[night_key], 1, 2) \n",
    "    current_eeg_tensor = torch.from_numpy(curent_eeg_data)\n",
    "\n",
    "    output = model_ft(current_eeg_tensor)\n",
    "    _, y_test_current_pred = torch.max(output, 1)\n",
    "    y_test_current_pred = y_test_current_pred.numpy()\n",
    "    \n",
    "    print(confusion_matrix(y_test_current, y_test_current_pred))\n",
    "\n",
    "    print(classification_report(y_test_current, y_test_current_pred, target_names=class_dict.values()))\n",
    "\n",
    "    print(\"Agreement score (Cohen Kappa): \", cohen_kappa_score(y_test_current, y_test_current_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "sleep_stages_pytorch.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
