{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K Nearest Neighbour Classification\n",
    "___\n",
    "\n",
    "This model aims to classify sleep stages based on two EEG channel. We will use the features extracted in the `pipeline.ipynb` notebook as the input to a KNN classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Ensure parent folder is in PYTHONPATH\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import joblib\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import (GridSearchCV,\n",
    "                                     RandomizedSearchCV,\n",
    "                                     GroupKFold,\n",
    "                                     cross_validate)\n",
    "from sklearn.metrics import (accuracy_score,\n",
    "                             confusion_matrix,\n",
    "                             classification_report,\n",
    "                             f1_score,\n",
    "                             cohen_kappa_score,\n",
    "                             make_scorer)\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from constants import (SLEEP_STAGES_VALUES,\n",
    "                       N_STAGES,\n",
    "                       EPOCH_DURATION)\n",
    "from model_utils import (print_hypnogram,\n",
    "                         train_test_split_one_subject,\n",
    "                         train_test_split_according_to_age,\n",
    "                         evaluate_hyperparams_grid,\n",
    "                         print_results_cv,\n",
    "                         print_results_cv_scores,\n",
    "                         get_pipeline,\n",
    "                         print_hyperparam_tuning_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the features\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(168954, 50)\n",
      "(168954,)\n",
      "Number of subjects:  78\n",
      "Number of nights:  153\n",
      "Subjects available:  [ 0.  1.  2.  3.  4.  5.  6.  7.  8.  9. 10. 11. 12. 13. 14. 15. 16. 17.\n",
      " 18. 19. 20. 21. 22. 23. 24. 25. 26. 27. 28. 29. 30. 31. 32. 33. 34. 35.\n",
      " 36. 37. 38. 40. 41. 42. 43. 44. 45. 46. 47. 48. 49. 50. 51. 52. 53. 54.\n",
      " 55. 56. 57. 58. 59. 60. 61. 62. 63. 64. 65. 66. 67. 70. 71. 72. 73. 74.\n",
      " 75. 76. 77. 80. 81. 82.]\n"
     ]
    }
   ],
   "source": [
    "# position of the subject information and night information in the X matrix\n",
    "SUBJECT_IDX = 0 \n",
    "NIGHT_IDX = 1\n",
    "USE_CONTINUOUS_AGE = False\n",
    "DOWNSIZE_SET = False\n",
    "TEST_SET_SUBJECTS = [0.0, 24.0, 49.0, 71.0]\n",
    "\n",
    "if USE_CONTINUOUS_AGE:\n",
    "    X_file_name = \"../data/x_features-age-continuous.npy\"\n",
    "    y_file_name = \"../data/y_observations-age-continuous.npy\"\n",
    "else:\n",
    "    X_file_name = \"../data/x_features.npy\"\n",
    "    y_file_name = \"../data/y_observations.npy\"\n",
    "    \n",
    "X_init = np.load(X_file_name, allow_pickle=True)\n",
    "y_init = np.load(y_file_name, allow_pickle=True)\n",
    "\n",
    "X_init = np.vstack(X_init)\n",
    "y_init = np.hstack(y_init)\n",
    "print(X_init.shape)\n",
    "print(y_init.shape)\n",
    "print(\"Number of subjects: \", np.unique(X_init[:,SUBJECT_IDX]).shape[0]) # Some subject indexes are skipped, thus total number is below 83 (as we can see in https://physionet.org/content/sleep-edfx/1.0.0/)\n",
    "print(\"Number of nights: \", len(np.unique([f\"{int(x[0])}-{int(x[1])}\" for x in X_init[:,SUBJECT_IDX:NIGHT_IDX+1]])))\n",
    "print(\"Subjects available: \", np.unique(X_init[:,SUBJECT_IDX]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected subjects for the test set are:  [0.0, 24.0, 49.0, 71.0]\n",
      "(8123, 50) (160831, 50) (8123,) (160831,)\n"
     ]
    }
   ],
   "source": [
    "X_test, X_train_valid, y_test, y_train_valid = train_test_split_according_to_age(\n",
    "    X_init,\n",
    "    y_init,\n",
    "    use_continuous_age=USE_CONTINUOUS_AGE,\n",
    "    subjects_test=TEST_SET_SUBJECTS)\n",
    "    \n",
    "print(X_test.shape, X_train_valid.shape, y_test.shape, y_train_valid.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN validation\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "NB_KFOLDS = 5\n",
    "NB_CATEGORICAL_FEATURES = 2\n",
    "NB_FEATURES = 48\n",
    "CLASSIFIER_PIPELINE_KEY = 'classifier'\n",
    "RANDOM_STATE = 42 \n",
    "\n",
    "def get_cv_iterator(n_splits=2):\n",
    "    return GroupKFold(n_splits=n_splits).split(\n",
    "        X_train_valid, groups=X_train_valid[:,SUBJECT_IDX]\n",
    "    )\n",
    "    \n",
    "def cross_validate_with_confusion_matrix(pipeline, n_fold):\n",
    "    accuracies = []\n",
    "    macro_f1_scores = []\n",
    "    weighted_f1_scores = []\n",
    "    kappa_agreements = []\n",
    "\n",
    "    for train_index, valid_index in get_cv_iterator(n_splits=n_fold):\n",
    "        # We drop the subject and night indexes\n",
    "        X_train, X_valid = X_train_valid[train_index, 2:], X_train_valid[valid_index, 2:]\n",
    "        y_train, y_valid = y_train_valid[train_index], y_train_valid[valid_index]\n",
    "\n",
    "        # Scaling features and model training\n",
    "        training_pipeline = pipeline\n",
    "        training_pipeline.fit(X_train, y_train)\n",
    "\n",
    "        # Validation\n",
    "        y_valid_pred = training_pipeline.predict(X_valid)\n",
    "\n",
    "        print(\"----------------------------- FOLD RESULTS --------------------------------------\\n\")\n",
    "        current_kappa = cohen_kappa_score(y_valid, y_valid_pred)\n",
    "\n",
    "        print(\"TRAIN:\", train_index, \"VALID:\", valid_index, \"\\n\\n\")\n",
    "        print(confusion_matrix(y_valid, y_valid_pred), \"\\n\")\n",
    "        print(classification_report(y_valid, y_valid_pred, target_names=SLEEP_STAGES_VALUES.keys()), \"\\n\")\n",
    "        print(\"Agreement score (Cohen Kappa): \", current_kappa, \"\\n\")\n",
    "\n",
    "        accuracies.append(round(accuracy_score(y_valid, y_valid_pred),2))\n",
    "        macro_f1_scores.append(f1_score(y_valid, y_valid_pred, average=\"macro\"))\n",
    "        weighted_f1_scores.append(f1_score(y_valid, y_valid_pred, average=\"weighted\"))\n",
    "        kappa_agreements.append(current_kappa)\n",
    "\n",
    "    print_results_cv(accuracies, macro_f1_scores, weighted_f1_scores, kappa_agreements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------- FOLD RESULTS --------------------------------------\n",
      "\n",
      "TRAIN: [  2137   2138   2139 ... 158843 158844 158845] VALID: [     0      1      2 ... 160828 160829 160830] \n",
      "\n",
      "\n",
      "[[6746  387  248   30  241]\n",
      " [ 950  688 1411    9  659]\n",
      " [1284  760 9460  655  779]\n",
      " [ 216   19  752 1938    6]\n",
      " [ 632 1066 1656    3 2209]] \n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           W       0.69      0.88      0.77      7652\n",
      "          N1       0.24      0.19      0.21      3717\n",
      "          N2       0.70      0.73      0.71     12938\n",
      "          N3       0.74      0.66      0.70      2931\n",
      "         REM       0.57      0.40      0.47      5566\n",
      "\n",
      "    accuracy                           0.64     32804\n",
      "   macro avg       0.58      0.57      0.57     32804\n",
      "weighted avg       0.62      0.64      0.63     32804\n",
      " \n",
      "\n",
      "Agreement score (Cohen Kappa):  0.5088390243817995 \n",
      "\n",
      "----------------------------- FOLD RESULTS --------------------------------------\n",
      "\n",
      "TRAIN: [     0      1      2 ... 160828 160829 160830] VALID: [  5807   5808   5809 ... 158843 158844 158845] \n",
      "\n",
      "\n",
      "[[ 6343   692   314    60   420]\n",
      " [  793   916  1194     5  1022]\n",
      " [  228   528 11208   463   934]\n",
      " [   35     3   580  1669     2]\n",
      " [  300   705  1069    10  2755]] \n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           W       0.82      0.81      0.82      7829\n",
      "          N1       0.32      0.23      0.27      3930\n",
      "          N2       0.78      0.84      0.81     13361\n",
      "          N3       0.76      0.73      0.74      2289\n",
      "         REM       0.54      0.57      0.55      4839\n",
      "\n",
      "    accuracy                           0.71     32248\n",
      "   macro avg       0.64      0.64      0.64     32248\n",
      "weighted avg       0.70      0.71      0.70     32248\n",
      " \n",
      "\n",
      "Agreement score (Cohen Kappa):  0.5958742117485885 \n",
      "\n",
      "----------------------------- FOLD RESULTS --------------------------------------\n",
      "\n",
      "TRAIN: [     0      1      2 ... 160828 160829 160830] VALID: [  2137   2138   2139 ... 151913 151914 151915] \n",
      "\n",
      "\n",
      "[[7268  888  607   74  577]\n",
      " [ 758 1004 1692   17 1305]\n",
      " [ 407 1226 9945  257 1216]\n",
      " [  13    2  279  724    6]\n",
      " [ 354  600 1067   18 1991]] \n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           W       0.83      0.77      0.80      9414\n",
      "          N1       0.27      0.21      0.24      4776\n",
      "          N2       0.73      0.76      0.75     13051\n",
      "          N3       0.66      0.71      0.68      1024\n",
      "         REM       0.39      0.49      0.44      4030\n",
      "\n",
      "    accuracy                           0.65     32295\n",
      "   macro avg       0.58      0.59      0.58     32295\n",
      "weighted avg       0.65      0.65      0.65     32295\n",
      " \n",
      "\n",
      "Agreement score (Cohen Kappa):  0.5063287134081166 \n",
      "\n",
      "----------------------------- FOLD RESULTS --------------------------------------\n",
      "\n",
      "TRAIN: [     0      1      2 ... 160828 160829 160830] VALID: [  4057   4058   4059 ... 121623 121624 121625] \n",
      "\n",
      "\n",
      "[[ 6027   693   238    34   282]\n",
      " [  651   721  1163    23   823]\n",
      " [  193   644 10762   554   739]\n",
      " [   67    14   726  2488     2]\n",
      " [  328   858  1266    12  2816]] \n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           W       0.83      0.83      0.83      7274\n",
      "          N1       0.25      0.21      0.23      3381\n",
      "          N2       0.76      0.83      0.80     12892\n",
      "          N3       0.80      0.75      0.78      3297\n",
      "         REM       0.60      0.53      0.57      5280\n",
      "\n",
      "    accuracy                           0.71     32124\n",
      "   macro avg       0.65      0.63      0.64     32124\n",
      "weighted avg       0.70      0.71      0.70     32124\n",
      " \n",
      "\n",
      "Agreement score (Cohen Kappa):  0.6022065920639418 \n",
      "\n",
      "----------------------------- FOLD RESULTS --------------------------------------\n",
      "\n",
      "TRAIN: [     0      1      2 ... 160828 160829 160830] VALID: [ 13884  13885  13886 ... 156772 156773 156774] \n",
      "\n",
      "\n",
      "[[5670  831  797  121  469]\n",
      " [ 644  888 1667    7  729]\n",
      " [ 315  907 9771  394  742]\n",
      " [  71   32  725 1814   14]\n",
      " [ 350 1042 1250    9 2101]] \n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           W       0.80      0.72      0.76      7888\n",
      "          N1       0.24      0.23      0.23      3935\n",
      "          N2       0.69      0.81      0.74     12129\n",
      "          N3       0.77      0.68      0.73      2656\n",
      "         REM       0.52      0.44      0.48      4752\n",
      "\n",
      "    accuracy                           0.65     31360\n",
      "   macro avg       0.60      0.58      0.59     31360\n",
      "weighted avg       0.64      0.65      0.64     31360\n",
      " \n",
      "\n",
      "Agreement score (Cohen Kappa):  0.5127429918161123 \n",
      "\n",
      "Mean accuracy          : 0.67 ± 0.031\n",
      "Mean macro F1-score    : 0.60 ± 0.029\n",
      "Mean weighted F1-score : 0.66 ± 0.033\n",
      "Mean Kappa's agreement : 0.55 ± 0.044\n",
      "CPU times: user 50min 36s, sys: 2.48 s, total: 50min 39s\n",
      "Wall time: 7min 13s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "cross_validate_with_confusion_matrix(get_pipeline(\n",
    "    classifier=KNeighborsClassifier(\n",
    "        n_jobs=-1\n",
    "    )\n",
    "), n_fold=NB_KFOLDS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the results are lower than the other models (SVC,RF), and takes much longer to train.\n",
    "\n",
    "```\n",
    "Mean accuracy          : 0.67 ± 0.031\n",
    "Mean macro F1-score    : 0.60 ± 0.029\n",
    "Mean weighted F1-score : 0.66 ± 0.032\n",
    "Mean Kappa's agreement : 0.55 ± 0.044\n",
    "CPU times: user 43min 2s, sys: 26.7 s, total: 43min 29s\n",
    "Wall time: 20min 36s\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.33 s, sys: 19.9 ms, total: 1.35 s\n",
      "Wall time: 1.35 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('scaling',\n",
       "                 ColumnTransformer(n_jobs=None, remainder='drop',\n",
       "                                   sparse_threshold=0.3,\n",
       "                                   transformer_weights=None,\n",
       "                                   transformers=[('pass-through-categorical',\n",
       "                                                  'passthrough', [0, 1]),\n",
       "                                                 ('scaling-continuous',\n",
       "                                                  StandardScaler(copy=False,\n",
       "                                                                 with_mean=True,\n",
       "                                                                 with_std=True),\n",
       "                                                  [2, 3, 4, 5, 6, 7, 8, 9, 10,\n",
       "                                                   11, 12, 13, 14, 15, 16, 17,\n",
       "                                                   18, 19, 20, 21, 22, 23, 24,\n",
       "                                                   25, 26, 27, 28, 29, 30, 31, ...])],\n",
       "                                   verbose=False)),\n",
       "                ('classifier',\n",
       "                 KNeighborsClassifier(algorithm='auto', leaf_size=30,\n",
       "                                      metric='minkowski', metric_params=None,\n",
       "                                      n_jobs=-1, n_neighbors=5, p=2,\n",
       "                                      weights='uniform'))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "knn_clf = get_pipeline(\n",
    "    classifier=KNeighborsClassifier(\n",
    "        n_jobs=-1)\n",
    ")\n",
    "\n",
    "knn_clf.fit(X_train_valid[:150000,2:], y_train_valid[:150000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3min 9s, sys: 140 ms, total: 3min 9s\n",
      "Wall time: 30 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([3, 2, 2, ..., 0, 0, 0])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "knn_clf.predict(X_train_valid[150000:,2:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see above, training time is really fast, while the prediction is quite slow. It can be explained by how the KNN model works: \n",
    "\n",
    "> [...] it does not attempt to construct a general internal model, but simply stores instances of the training data. Classification is computed from a simple majority vote of the nearest neighbors of each point: a query point is assigned the data class which has the most representatives within the nearest neighbors of the point.\n",
    "\n",
    "## Validation results\n",
    "___\n",
    "\n",
    "### Dimension reduction\n",
    "___\n",
    "\n",
    "As with the other models, we will use LDA and PCA to reduce dimensions. We will try the maintain the same scores, and reduce the time it takes to cross-validate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validate_with_dim_reduction(dim_reduction):\n",
    "    pipeline = get_pipeline(\n",
    "        classifier=KNeighborsClassifier(n_jobs=-1),\n",
    "        dimension_reduction=dim_reduction\n",
    "    )\n",
    "    \n",
    "    scores = cross_validate(\n",
    "        estimator=pipeline,\n",
    "        X=X_train_valid,\n",
    "        y=y_train_valid,\n",
    "        groups=X_train_valid[:,SUBJECT_IDX],\n",
    "        scoring={\n",
    "            \"agreement\": make_scorer(cohen_kappa_score),\n",
    "            \"accuracy\": 'accuracy',\n",
    "            \"f1-score-macro\": 'f1_macro',\n",
    "            \"f1-score-weighted\": 'f1_weighted',\n",
    "        },\n",
    "        cv=get_cv_iterator(n_splits=5),\n",
    "        verbose=1,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    print_results_cv_scores(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. LDA\n",
    "___\n",
    "\n",
    "We have `n_components=4` by default when using LDA to reduce dimensionality. We can see it speeds up a lot (15.8s with vs ~20min without) the prediction time for the same number of folds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   5 | elapsed:    6.7s remaining:   10.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean accuracy          : 0.66 ± 0.031\n",
      "Mean macro F1-score    : 0.59 ± 0.024\n",
      "Mean weighted F1-score : 0.65 ± 0.028\n",
      "Mean Kappa's agreement : 0.53 ± 0.040\n",
      "CPU times: user 57.3 ms, sys: 83.8 ms, total: 141 ms\n",
      "Wall time: 7.14 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:    7.1s finished\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "cross_validate_with_dim_reduction(LinearDiscriminantAnalysis())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. PCA\n",
    "___\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   5 | elapsed:    6.7s remaining:   10.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean accuracy          : 0.50 ± 0.022\n",
      "Mean macro F1-score    : 0.45 ± 0.009\n",
      "Mean weighted F1-score : 0.50 ± 0.020\n",
      "Mean Kappa's agreement : 0.32 ± 0.025\n",
      "CPU times: user 39.9 ms, sys: 31.9 ms, total: 71.8 ms\n",
      "Wall time: 7.02 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:    7.0s finished\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "cross_validate_with_dim_reduction(PCA(n_components=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   5 | elapsed:   26.2s remaining:   39.2s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean accuracy          : 0.63 ± 0.037\n",
      "Mean macro F1-score    : 0.57 ± 0.033\n",
      "Mean weighted F1-score : 0.63 ± 0.035\n",
      "Mean Kappa's agreement : 0.49 ± 0.051\n",
      "CPU times: user 36.2 ms, sys: 16 ms, total: 52.1 ms\n",
      "Wall time: 27.7 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:   27.7s finished\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "cross_validate_with_dim_reduction(PCA(n_components=16))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   5 | elapsed:   59.1s remaining:  1.5min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean accuracy          : 0.63 ± 0.036\n",
      "Mean macro F1-score    : 0.57 ± 0.032\n",
      "Mean weighted F1-score : 0.63 ± 0.034\n",
      "Mean Kappa's agreement : 0.50 ± 0.048\n",
      "CPU times: user 43.5 ms, sys: 20 ms, total: 63.4 ms\n",
      "Wall time: 1min 2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:  1.0min finished\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "cross_validate_with_dim_reduction(PCA(n_components=35))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results\n",
    "___\n",
    "\n",
    "|  Score             |  without      | LDA           | PCA (n_comp=4) | PCA (n_comp=16) | PCA (n_comp=35) |\n",
    "|--------------------|---------------|---------------|----------------|-----------------|-----------------|\n",
    "|  accuracy          | 0.67 ± 0.031  |  0.66 ± 0.031 | 0.50 ± 0.022   |  0.63 ± 0.037   |  0.63 ± 0.036   |\n",
    "|  macro F1-score    | 0.60 ± 0.029  |  0.59 ± 0.024 | 0.45 ± 0.009   |  0.57 ± 0.033   |  0.57 ± 0.032   |\n",
    "|  weighted F1-score | 0.66 ± 0.032  |  0.65 ± 0.028 | 0.50 ± 0.020   |  0.63 ± 0.035   |  0.63 ± 0.034   |\n",
    "|  Kappa's agreement | 0.55 ± 0.044  |  0.53 ± 0.040 | 0.32 ± 0.025   |  0.49 ± 0.050   |  0.50 ± 0.048   |\n",
    "|  Time              | 20min 36s     |  15.8 s       | 15.3 s         |  52.4 s         |  1min 42s       |\n",
    "\n",
    "The results with LDA used as dimension reduction have slightly worst results thant without, but has the best overall score accross PCA and LDA scores. We will keep LDA as a step in our pipeline, because it speeds up a lot prediction time. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters tuning\n",
    "___\n",
    "\n",
    "The hyperparameters of a KNN classifier are:\n",
    "- `n_neighbors`: `int` (`default=5`)\n",
    "    \n",
    "    Represents the number of neighbors which votes the predicted class\n",
    "- `weights`: `{‘uniform’, ‘distance’} or callable, default='uniform'`\n",
    "\n",
    "    Weight functions attributed to neighbors in the voting process. If `uniform`, all neighbors have the same weight, whilst setting `weights` to `distance` involves that closer neighbors of a query point will have a greater influence than neighbors which are further away.\n",
    "\n",
    "- `leaf_size`: `positive int` (`default=30`)\n",
    "\n",
    "    Number of points at which to switch to brute-force. Changing leaf_size will not affect the results of a query, but can significantly impact the speed of a query and the memory required to store the constructed tree. **The optimal value depends on the nature of the problem.**\n",
    "    \n",
    "- `metric`: `str or callable, default=’minkowski’`\n",
    "\n",
    "    The distance metric to use for the tree. The default metric is minkowski, and with p=2 is equivalent to the standard Euclidean metric. \n",
    "    With hyperparam `p`, default value is euclidean distance. We could also look at `manhattan` and `chebyshev`.\n",
    "    \n",
    "### 1. Hyperparameters `n_neighbors` and `weights`\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "evaluate_hyperparams_grid(\n",
    "    params={\n",
    "        f\"{CLASSIFIER_PIPELINE_KEY}__n_neighbors\": np.linspace(200, 500, 10, dtype=\"int\"),\n",
    "        f\"{CLASSIFIER_PIPELINE_KEY}__weights\": ['uniform'],\n",
    "\n",
    "    },\n",
    "    estimator=get_pipeline(\n",
    "        classifier=KNeighborsClassifier(n_jobs=-1),\n",
    "        dimension_reduction=LinearDiscriminantAnalysis()\n",
    "    ),\n",
    "    X=X_train_valid,\n",
    "    y=y_train_valid,\n",
    "    cv=get_cv_iterator(n_splits=2),\n",
    "    use_randomized=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1st experiment**: `n_neighbors\": [  2,  57, 112, 168, 223, 278, 334, 389, 444, 500]` with `weights=['uniform', 'distance']`\n",
    "\n",
    "|Rank| n_neighbors      |  weights          | Test score     |\n",
    "|----|------------------|-------------------|----------------|\n",
    "|1   | 389              | 'uniform'         | 0.5815 ± 0.001 |\n",
    "|2   | 223              | 'uniform'         | 0.5814 ± 0.002 |\n",
    "|3   | 334              | 'uniform'         | 0.5813 ± 0.002 |\n",
    "|4   | 500              | 'uniform'         | 0.5812 ± 0.002 |\n",
    "| ... |\n",
    "|13   | 57               | 'distance'        | 0.5766 ± 0.004 |\n",
    "|14   | 2                | 'distance'        | 0.4655 ± 0.001 |\n",
    "|15   | 2                | 'uniform'         | 0.4589 ± 0.014 |\n",
    "\n",
    "1st run:\n",
    "\n",
    "1. Parameter {'classifier__weights': 'uniform', 'classifier__n_neighbors': 389} has a score of 0.5815 ± 0.001\n",
    "2. Parameter {'classifier__weights': 'uniform', 'classifier__n_neighbors': 223} has a score of 0.5814 ± 0.002\n",
    "3. Parameter {'classifier__weights': 'uniform', 'classifier__n_neighbors': 500} has a score of 0.5812 ± 0.002\n",
    "4. Parameter {'classifier__weights': 'distance', 'classifier__n_neighbors': 500} has a score of 0.5810 ± 0.002\n",
    "5. Parameter {'classifier__weights': 'distance', 'classifier__n_neighbors': 223} has a score of 0.5809 ± 0.003\n",
    "6. Parameter {'classifier__weights': 'distance', 'classifier__n_neighbors': 168} has a score of 0.5806 ± 0.003\n",
    "7. Parameter {'classifier__weights': 'distance', 'classifier__n_neighbors': 112} has a score of 0.5797 ± 0.004\n",
    "8. Parameter {'classifier__weights': 'uniform', 'classifier__n_neighbors': 57} has a score of 0.5774 ± 0.005\n",
    "9. Parameter {'classifier__weights': 'distance', 'classifier__n_neighbors': 2} has a score of 0.4655 ± 0.001\n",
    "10. Parameter {'classifier__weights': 'uniform', 'classifier__n_neighbors': 2} has a score of 0.4589 ± 0.014\n",
    "\n",
    "2nd run (without duplicates):\n",
    "\n",
    "3. Parameter {'classifier__weights': 'uniform', 'classifier__n_neighbors': 334} has a score of 0.5813 ± 0.002\n",
    "4. Parameter {'classifier__weights': 'uniform', 'classifier__n_neighbors': 168} has a score of 0.5810 ± 0.003\n",
    "5. Parameter {'classifier__weights': 'distance', 'classifier__n_neighbors': 444} has a score of 0.5809 ± 0.001\n",
    "6. Parameter {'classifier__weights': 'uniform', 'classifier__n_neighbors': 112} has a score of 0.5807 ± 0.004\n",
    "9. Parameter {'classifier__weights': 'distance', 'classifier__n_neighbors': 57} has a score of 0.5766 ± 0.004\n",
    "\n",
    "We can see that the best results comes with uniform weights and `n_neighbors` in the range [200,500].\n",
    "\n",
    "___\n",
    "\n",
    "**2nd experiment**: `n_neighbors\": [200, 233, 266, 300, 333, 366, 400, 433, 466, 500]` with `weights=['uniform']`\n",
    "\n",
    "|Rank| n_neighbors      |  weights          | Test score     |\n",
    "|----|------------------|-------------------|----------------|\n",
    "|1   | 200              | 'uniform'         | 0.5817 ± 0.003 |\n",
    "|2   | 300              | 'uniform'         | 0.5817 ± 0.001 |\n",
    "|3   | 233              | 'uniform'         | 0.5816 ± 0.002 |\n",
    "|4   | 400              | 'uniform'         | 0.5815 ± 0.002 |\n",
    "| ... |\n",
    "|9    | 266              | 'uniform'         | 0.5812 ± 0.002 |\n",
    "|10   | 366              | 'uniform'         | 0.5812 ± 0.001 |\n",
    "\n",
    "1. Parameter {'classifier__weights': 'uniform', 'classifier__n_neighbors': 200} has a score of 0.5817 ± 0.003\n",
    "2. Parameter {'classifier__weights': 'uniform', 'classifier__n_neighbors': 300} has a score of 0.5817 ± 0.001\n",
    "3. Parameter {'classifier__weights': 'uniform', 'classifier__n_neighbors': 233} has a score of 0.5816 ± 0.002\n",
    "4. Parameter {'classifier__weights': 'uniform', 'classifier__n_neighbors': 400} has a score of 0.5815 ± 0.002\n",
    "5. Parameter {'classifier__weights': 'uniform', 'classifier__n_neighbors': 466} has a score of 0.5814 ± 0.002\n",
    "6. Parameter {'classifier__weights': 'uniform', 'classifier__n_neighbors': 433} has a score of 0.5813 ± 0.001\n",
    "7. Parameter {'classifier__weights': 'uniform', 'classifier__n_neighbors': 500} has a score of 0.5812 ± 0.002\n",
    "8. Parameter {'classifier__weights': 'uniform', 'classifier__n_neighbors': 333} has a score of 0.5812 ± 0.002\n",
    "9. Parameter {'classifier__weights': 'uniform', 'classifier__n_neighbors': 266} has a score of 0.5812 ± 0.002\n",
    "10. Parameter {'classifier__weights': 'uniform', 'classifier__n_neighbors': 366} has a score of 0.5812 ± 0.001\n",
    "CPU times: user 3.19 s, sys: 737 ms, total: 3.92 s\n",
    "Wall time: 3min 30s\n",
    "\n",
    "**We will keep `weights: 'uniform'`, `n_neighbors': 200`.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Hyperparameter `leaf_size`\n",
    "____\n",
    "\n",
    "We will try to find the value that minimizes prediction time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search = GridSearchCV(\n",
    "    estimator=get_pipeline(\n",
    "        classifier=KNeighborsClassifier(n_jobs=-1),\n",
    "        dimension_reduction=LinearDiscriminantAnalysis()\n",
    "    ),\n",
    "    param_grid={\n",
    "        f\"{CLASSIFIER_PIPELINE_KEY}__leaf_size\": np.linspace(10, 100, 10, dtype=\"int\")\n",
    "    },\n",
    "    scoring=make_scorer(cohen_kappa_score),\n",
    "    cv=get_cv_iterator(n_splits=5),\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "search.fit(X_train_valid[:,2:], y_train_valid);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, current_param in enumerate(search.cv_results_['params']):\n",
    "    mean_fit_time = search.cv_results_['mean_fit_time'][idx]\n",
    "    std_fit_time = search.cv_results_['std_fit_time'][idx]\n",
    "    mean_score_time = search.cv_results_['mean_score_time'][idx]\n",
    "    std_score_time = search.cv_results_['std_score_time'][idx]\n",
    "    print(f\"Parameter {current_param} has a score time of {mean_score_time:0.4f}s ± {std_score_time:0.3f}s and fit time of {mean_fit_time:0.4f} ± {std_fit_time:0.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1st experiment**: `leaf_size=[10,20,30,...,90,100]`\n",
    "\n",
    "|Rank| leaf_size     |  Score time       | Fit time       |\n",
    "|----|---------------|-------------------|----------------|\n",
    "|1   | 100           | 3.0002s ± 1.039s  | 4.0782 ± 0.958 |\n",
    "|2   | 60            | 3.1930s ± 0.377s  | 4.8671 ± 0.521 |\n",
    "|3   | 40            | 3.9384s ± 0.762s  | 6.5524 ± 0.852 |\n",
    "|4   | 80            | 3.9545s ± 0.686s  | 5.0560 ± 0.466 |\n",
    "| ... |\n",
    "|9   | 30            | 4.4972s ± 0.774s  | 7.0859 ± 0.918 |\n",
    "|10  | 20            | 5.0833s ± 0.358s  | 6.9917 ± 0.407 |\n",
    "\n",
    "We will choose `leaf_size=100`.\n",
    "\n",
    "### 3. `metric` hyperparameter\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "evaluate_hyperparams_grid(\n",
    "    params={\n",
    "        f\"{CLASSIFIER_PIPELINE_KEY}__metric\": ['minkowski', 'manhattan', 'chebyshev']\n",
    "    },\n",
    "    estimator=get_pipeline(\n",
    "        classifier=KNeighborsClassifier(n_jobs=-1),\n",
    "        dimension_reduction=LinearDiscriminantAnalysis()\n",
    "    ),\n",
    "    X=X_train_valid,\n",
    "    y=y_train_valid,\n",
    "    cv=get_cv_iterator(n_splits=5),\n",
    "    use_randomized=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|Rank| metric        |  Score          |\n",
    "|----|---------------|-----------------|\n",
    "|1   | 'manhattan'   | 0.5348 ± 0.038  |\n",
    "|2   | 'chebyshev'   | 0.5336 ± 0.038  |\n",
    "|3   | 'minkowski'   | 0.5333 ± 0.039  |\n",
    "\n",
    "We will choose `metric='manhattan'`.\n",
    "\n",
    "### Checking final hyperparameters\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "evaluate_hyperparams_grid(\n",
    "    params={\n",
    "        f\"{CLASSIFIER_PIPELINE_KEY}__metric\": ['manhattan', 'chebyshev'],\n",
    "        f\"{CLASSIFIER_PIPELINE_KEY}__n_neighbors\": np.linspace(200, 300, 3, dtype=\"int\"),\n",
    "        f\"{CLASSIFIER_PIPELINE_KEY}__weights\": ['uniform'],\n",
    "        f\"{CLASSIFIER_PIPELINE_KEY}__leaf_size\": [200]\n",
    "\n",
    "    },\n",
    "    estimator=get_pipeline(\n",
    "        classifier=KNeighborsClassifier(n_jobs=-1),\n",
    "        dimension_reduction=LinearDiscriminantAnalysis()\n",
    "    ),\n",
    "    X=X_train_valid,\n",
    "    y=y_train_valid,\n",
    "    cv=get_cv_iterator(n_splits=5),\n",
    "    use_randomized=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|Rank| n_neighbors   |  metric           | Score          |\n",
    "|----|---------------|-------------------|----------------|\n",
    "|1   | 300           | chebyshev         | 0.5820 ± 0.044 |\n",
    "|2   | 250           | manhattan         | 0.5819 ± 0.044 |\n",
    "|3   | 250           | chebyshev         | 0.5817 ± 0.044 |\n",
    "|4   | 200           | manhattan         | 0.5815 ± 0.044 |\n",
    "|5   | 300           | manhattan         | 0.5814 ± 0.044 |\n",
    "|6   | 200           | chebyshev         | 0.5813 ± 0.044 |\n",
    "\n",
    "We had previously set the hyperparamters the following value, by independantly running tests:\n",
    "```\n",
    "weights='uniform',\n",
    "n_neighbors=200,\n",
    "leaf_size=100,\n",
    "metric='manhattan',\n",
    "```\n",
    "\n",
    "By checking all these together, with runner ups for hyperparameters `n_neighbors` and `metric`, we found that the following hyperparameters should be better:\n",
    "```\n",
    "weights='uniform',\n",
    "n_neighbors=300,\n",
    "leaf_size=100,\n",
    "metric='chebyshev',\n",
    "```\n",
    "\n",
    "## Testing \n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.72 s, sys: 1.54 s, total: 6.26 s\n",
      "Wall time: 1.99 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('scaling',\n",
       "                 ColumnTransformer(n_jobs=None, remainder='drop',\n",
       "                                   sparse_threshold=0.3,\n",
       "                                   transformer_weights=None,\n",
       "                                   transformers=[('pass-through-categorical',\n",
       "                                                  'passthrough', [0, 1]),\n",
       "                                                 ('scaling-continuous',\n",
       "                                                  StandardScaler(copy=False,\n",
       "                                                                 with_mean=True,\n",
       "                                                                 with_std=True),\n",
       "                                                  [2, 3, 4, 5, 6, 7, 8, 9, 10,\n",
       "                                                   11, 12, 13, 14, 15, 16, 17,\n",
       "                                                   18, 19, 20, 21, 22, 23, 24,\n",
       "                                                   25, 26, 27, 28, 29, 30, 31, ...])],\n",
       "                                   verbose=False)),\n",
       "                ('dimension_reduction',\n",
       "                 LinearDiscriminantAnalysis(n_components=None, priors=None,\n",
       "                                            shrinkage=None, solver='svd',\n",
       "                                            store_covariance=False,\n",
       "                                            tol=0.0001)),\n",
       "                ('classifier',\n",
       "                 KNeighborsClassifier(algorithm='auto', leaf_size=100,\n",
       "                                      metric='chebyshev', metric_params=None,\n",
       "                                      n_jobs=-1, n_neighbors=300, p=2,\n",
       "                                      weights='uniform'))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "testing_pipeline = get_pipeline(\n",
    "    classifier=KNeighborsClassifier(\n",
    "        weights='uniform',\n",
    "        n_neighbors=300,\n",
    "        leaf_size=100,\n",
    "        metric='chebyshev',\n",
    "        n_jobs=-1\n",
    "    ),\n",
    "    dimension_reduction=LinearDiscriminantAnalysis()\n",
    ")\n",
    "\n",
    "testing_pipeline.fit(X_train_valid[:, 2:], y_train_valid);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1490    9    7   29   89]\n",
      " [ 317  145  319    6  196]\n",
      " [  80   84 3163  193   83]\n",
      " [   4    0   46  561    0]\n",
      " [ 107  106  295    3  791]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           W       0.75      0.92      0.82      1624\n",
      "          N1       0.42      0.15      0.22       983\n",
      "          N2       0.83      0.88      0.85      3603\n",
      "          N3       0.71      0.92      0.80       611\n",
      "         REM       0.68      0.61      0.64      1302\n",
      "\n",
      "    accuracy                           0.76      8123\n",
      "   macro avg       0.68      0.69      0.67      8123\n",
      "weighted avg       0.73      0.76      0.73      8123\n",
      "\n",
      "Agreement score (Cohen Kappa):  0.6561374430741804\n"
     ]
    }
   ],
   "source": [
    "y_test_pred = testing_pipeline.predict(X_test[:,2:])\n",
    "\n",
    "print(confusion_matrix(y_test, y_test_pred))\n",
    "\n",
    "print(classification_report(y_test, y_test_pred, target_names=SLEEP_STAGES_VALUES.keys()))\n",
    "\n",
    "print(\"Agreement score (Cohen Kappa): \", cohen_kappa_score(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test results\n",
    "___\n",
    "\n",
    "#### a) With LDA and tuning (metric=manhattan and n_neighbors=200)\n",
    "___\n",
    "\n",
    "```\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           W       0.75      0.92      0.83      1624\n",
    "          N1       0.44      0.15      0.23       983\n",
    "          N2       0.83      0.87      0.85      3603\n",
    "          N3       0.70      0.92      0.80       611\n",
    "         REM       0.68      0.61      0.64      1302\n",
    "\n",
    "    accuracy                           0.76      8123\n",
    "   macro avg       0.68      0.70      0.67      8123\n",
    "weighted avg       0.73      0.76      0.73      8123\n",
    "\n",
    "Agreement score (Cohen Kappa):  0.656774449223104\n",
    "```\n",
    "\n",
    "#### b) With LDA and tuning (metric=chebyshev and n_neighbors=300)\n",
    "\n",
    "```\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           W       0.75      0.92      0.82      1624\n",
    "          N1       0.43      0.15      0.22       983\n",
    "          N2       0.83      0.88      0.85      3603\n",
    "          N3       0.71      0.92      0.80       611\n",
    "         REM       0.68      0.61      0.64      1302\n",
    "\n",
    "    accuracy                           0.76      8123\n",
    "   macro avg       0.68      0.69      0.67      8123\n",
    "weighted avg       0.73      0.76      0.73      8123\n",
    "\n",
    "Agreement score (Cohen Kappa):  0.6566457163531443\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving trained model\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVED_DIR = \"../trained_model\"\n",
    "\n",
    "if not os.path.exists(SAVED_DIR):\n",
    "    os.mkdir(SAVED_DIR);\n",
    "\n",
    "if USE_CONTINUOUS_AGE: \n",
    "    joblib.dump(testing_pipeline, f\"{SAVED_DIR}/classifier_KNN_age_continuous.joblib\")\n",
    "else:\n",
    "    joblib.dump(testing_pipeline, f\"{SAVED_DIR}/classifier_KNN.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
