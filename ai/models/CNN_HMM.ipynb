{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sleep Stage Scoring (on raw EEG signal)\n",
    "\n",
    "What is different from [this version](https://github.com/baroquerock/sleep-stages-scoring):\n",
    "1. Pytorch framework\n",
    "2. Multichannel input\n",
    "3. Different architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "New architecture accomodates two channels:\n",
    "\n",
    "<img src=\"img/arch2.png\" width=\"800\" height=\"1000\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import copy\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch import optim, nn\n",
    "from torchvision import transforms, datasets, models\n",
    "\n",
    "from collections import OrderedDict\n",
    "\n",
    "from npzloader import DataLoader as DataLoader_x\n",
    "import torch.utils.data as utils\n",
    "\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.metrics import (confusion_matrix, classification_report, cohen_kappa_score)\n",
    "\n",
    "from hmmlearn.hmm import MultinomialHMM\n",
    "\n",
    "from model_utils import print_hypnogram\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "moB1UfGjD7vW"
   },
   "outputs": [],
   "source": [
    "# loading fpz_cz and pz_oz channels\n",
    "loader = DataLoader_x(\"../data/eeg_fpz_cz\")\n",
    "data_fp, labels = loader.load_data(verbose=0)\n",
    "data_fp = {''.join(k.partition('fpz_cz')[1:]) : v for k, v in data_fp.items()} \n",
    "\n",
    "loader = DataLoader_x(\"../data/eeg_pz_oz\")\n",
    "data_pz, _ = loader.load_data(verbose=0)\n",
    "data_pz = {''.join(k.partition('pz_oz')[1:]) : v for k, v in data_pz.items()} \n",
    "\n",
    "all_labels = np.hstack(labels.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sSFEK863D7ve"
   },
   "outputs": [],
   "source": [
    "# Sleep stages values\n",
    "W = 0\n",
    "N1 = 1\n",
    "N2 = 2\n",
    "N3 = 3\n",
    "REM = 4\n",
    "\n",
    "# int-to-label map\n",
    "class_dict = {W: \"W\", \n",
    "              N1: \"N1\",\n",
    "              N2: \"N2\",\n",
    "              N3: \"N3\",\n",
    "              REM: \"REM\"}\n",
    "\n",
    "TEST_SUBJECT_INDEX = \"09\"\n",
    "TEST_SUBJECT_FILE_PREFIX = f\"SC4{TEST_SUBJECT_INDEX}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PbmyneTYD7wv"
   },
   "source": [
    "### TRAINING\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uegNojzFD7w2"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Function to randomly select subjects for partial cross-validation and prepare the data for training.\n",
    "Train and test sets should be independent.\n",
    "A patient with is SC413 is excluded from any test set because this patient has only one recording.\n",
    "\"\"\"\n",
    "def get_cv_data(data, labels, n_folds=3, ch='fpz_cz'):\n",
    "    \n",
    "    random.seed(29)\n",
    "    test_key = []\n",
    "    keys = list(labels.keys())\n",
    "    keys = list(set([x[:5] for x in keys if '413' not in x]))\n",
    "    for i in range(len(keys)):\n",
    "        if f\"4{TEST_SUBJECT_INDEX}\" in keys[i]: \n",
    "            test_key = [keys[i]]\n",
    "    random.shuffle(keys)\n",
    "    print(test_key)\n",
    "    val_keys = keys[:n_folds]\n",
    " \n",
    "    cv_data = []\n",
    "    test_key = [test_key[0] + '1E0',test_key[0] + '2E0']\n",
    "    \n",
    "    for fold in val_keys:\n",
    "        fold_keys = [fold+'1E0', fold+'2E0']\n",
    "        \n",
    "        val_labels = [labels[key] for key in fold_keys]\n",
    "        val_labels = np.hstack(val_labels)\n",
    "        \n",
    "        keys_not_training = fold_keys + test_key\n",
    "        \n",
    "        train_labels = [labels[key] for key in labels.keys() if key not in keys_not_training]\n",
    "        train_labels = np.hstack(train_labels)\n",
    "       \n",
    "        test_labels=[labels[key] for key in test_key]\n",
    "        test_labels= np.hstack(test_labels)\n",
    "        \n",
    "        if ch:\n",
    "            val_data = [data['{}/{}'.format(ch,key)] for key in fold_keys]\n",
    "            train_data = [data['{}/{}'.format(ch,key)] for key in labels.keys() if key not in keys_not_training]   \n",
    "            test_data = [data['{}/{}'.format(ch,key)] for key in test_key]\n",
    "        else:\n",
    "            val_data = [data[key] for key in fold_keys]\n",
    "            train_data = [data[key] for key in labels.keys() if key not in keys_not_training]\n",
    "            test_data = [data[key] for key in labels.keys() if key in test_key]\n",
    "           \n",
    "        train_data = np.vstack(train_data)\n",
    "        val_data = np.vstack(val_data)\n",
    "        test_data = np.vstack(test_data)\n",
    "    \n",
    "        cv_data.append((train_data, train_labels, val_data, val_labels, test_data, test_labels))\n",
    "    \n",
    "    return cv_data \n",
    "\n",
    "\"\"\"\n",
    "Function to slide on array with a moving window of size n\n",
    "\"\"\"\n",
    "\n",
    "def _slide(data, labels, n=3):\n",
    "    \n",
    "    #zero-indexing\n",
    "    data = [data[i:i+n] for i, _ in enumerate(data[n:])]\n",
    "    data = [np.vstack(x) for x in data]\n",
    "    data = np.array(data)\n",
    "    \n",
    "    labels = labels[n:]\n",
    "    \n",
    "    assert data.shape[0] == labels.shape[0]\n",
    "    \n",
    "    return data, labels\n",
    "\n",
    "\"\"\"\n",
    "Function to do batch processing with _slide()\n",
    "\"\"\"\n",
    "\n",
    "def batch_slide(data_dict, labels_dict, n=3):\n",
    "    \n",
    "    keys = [(x,y) for x,y in zip(data_dict, labels_dict)]\n",
    "    data = [_slide(data_dict[x], labels_dict[y], n=n) for x,y in keys]\n",
    "    \n",
    "    data_keys, label_keys = zip(*keys)\n",
    "    data, labels = zip(*data)\n",
    "    \n",
    "    data_dict = {k:v for k,v in zip(data_keys, data)}\n",
    "    data_dict = OrderedDict(sorted(data_dict.items()))\n",
    "    \n",
    "    labels_dict = {k:v for k,v in zip(label_keys, labels)}\n",
    "    labels_dict = OrderedDict(sorted(labels_dict.items()))\n",
    "        \n",
    "    return data_dict, labels_dict \n",
    "  \n",
    "  \n",
    "\"\"\"\n",
    "Function to merge channels into single recording.\n",
    "axis = 0: merge channels along the first dimension - (3000,1) and (3000,1) --> (6000,1)\n",
    "axis = 0: merge channels along the second dimension - (3000,1) and (3000,1) --> (3000,2)\n",
    "\"\"\"  \n",
    "    \n",
    "def merge_channels(channels, axis=0):\n",
    "    \n",
    "    # number of records should be the same in all channels\n",
    "    assert len(set([len(r) for r in channels])) == 1\n",
    "    \n",
    "    keys = [ch.keys() for ch in channels]\n",
    "    \n",
    "    merged = {}\n",
    "    \n",
    "    for records in zip(*keys):\n",
    "        \n",
    "        # assert that all recordings are for the same patient\n",
    "        assert len(set([r.split(os.sep)[1] for r in records])) == 1\n",
    "        vals = [ch[r] for r, ch in zip(records, channels)]\n",
    "        if axis:\n",
    "            stacked = np.dstack(vals)\n",
    "        else:\n",
    "            stacked = np.hstack(vals)\n",
    "        patient = records[0].split(os.sep)[1]\n",
    "        merged[patient] = stacked\n",
    "        \n",
    "    merged = OrderedDict(sorted(merged.items()))\n",
    "    return merged\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xb01HsHBoraY"
   },
   "outputs": [],
   "source": [
    "data_all = merge_channels([data_fp, data_pz], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in ['SC4101E0', 'SC4102E0', 'SC4111E0', 'SC4112E0', 'SC4121E0', 'SC4122E0', 'SC4131E0', 'SC4141E0', 'SC4142E0', 'SC4151E0', 'SC4152E0', 'SC4161E0', 'SC4162E0', 'SC4171E0', 'SC4172E0', 'SC4181E0', 'SC4182E0', 'SC4191E0', 'SC4192E0']:\n",
    "    del data_all[key]\n",
    "    del labels[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IEsk57FID7xH"
   },
   "outputs": [],
   "source": [
    "# getting cross-valivation data\n",
    "# note: n_folds is not a number of folds in traditional sense\n",
    "# the model is trained on 19 patients and validated on 1,\n",
    "# so n_folds is a number of patients for validation\n",
    "# to perform full cross-validation n_folds should be equal to 19 \n",
    "# (not 20, because for SC413 patient there is only one record)\n",
    "cv_data = get_cv_data(data_all, labels, ch=None, n_folds=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Otf2uGzLD7yi"
   },
   "outputs": [],
   "source": [
    "def small_conv(fs): \n",
    "    return nn.Sequential(\n",
    "            nn.Conv1d(1, 64, kernel_size=fs//2, stride=fs//16, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.MaxPool1d(kernel_size=8, stride=8),\n",
    "            \n",
    "            nn.Conv1d(64, 64, kernel_size=8, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(64, 64, kernel_size=8, padding=2),\n",
    "            nn.ReLU(), \n",
    "            nn.Conv1d(64, 64, kernel_size=8, padding=2),\n",
    "            nn.ReLU())\n",
    "\n",
    "def big_conv(fs): \n",
    "    return nn.Sequential(\n",
    "            nn.Conv1d(1, 64, kernel_size=fs*4, stride=fs//2, padding=2),\n",
    "            nn.ReLU(), \n",
    "            nn.Dropout(0.3),\n",
    "            nn.MaxPool1d(kernel_size=4, stride=4),\n",
    "            \n",
    "            nn.Conv1d(64, 64, kernel_size=6, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(64, 64, kernel_size=6, padding=2),\n",
    "            nn.ReLU(), \n",
    "            nn.Conv1d(64, 64, kernel_size=6, padding=2),\n",
    "            nn.ReLU())\n",
    "\n",
    "\n",
    "class ConvNet(nn.Module):\n",
    "  \n",
    "    def __init__(self, n_cnn_dense=256, fs=100, num_classes=5):\n",
    "      \n",
    "        super(ConvNet, self).__init__()\n",
    "        \n",
    "        self.layer1_fz = small_conv(fs)        \n",
    "        self.layer2_fz = big_conv(fs)\n",
    "\n",
    "        self.layer1_pz = small_conv(fs)        \n",
    "        self.layer2_pz = big_conv(fs)\n",
    "        \n",
    "        \n",
    "        self.fc1 = nn.Sequential(\n",
    "            nn.Linear(124, n_cnn_dense),\n",
    "            nn.ReLU(),            \n",
    "            nn.MaxPool1d(kernel_size=4, stride=4))\n",
    "        \n",
    "        self.fc2 = nn.Sequential(\n",
    "            nn.Linear(4096, num_classes),\n",
    "            nn.LogSoftmax(dim=1))\n",
    "        \n",
    "                \n",
    "        \n",
    "    def forward(self, channels):\n",
    "      \n",
    "        ch1 = channels[:, 0, :].unsqueeze(1)\n",
    "        ch2 = channels[:, 1, :].unsqueeze(1)\n",
    "        \n",
    "        out1_1 = self.layer1_fz(ch1)\n",
    "        out2_1 = self.layer2_fz(ch1)\n",
    "        \n",
    "        out1_2 = self.layer1_pz(ch2)\n",
    "        out2_2 = self.layer2_pz(ch2)        \n",
    "        \n",
    "        out = torch.cat((out1_1, out2_1, out1_2, out2_2), dim=2)\n",
    "        out = self.fc1(out)\n",
    "        s = out.size()[0]\n",
    "        out = out.view(s, -1)\n",
    "        out = self.fc2(out)\n",
    "       \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zTBFZaDKne0p"
   },
   "outputs": [],
   "source": [
    "model = ConvNet()\n",
    "train_data, train_labels, val_data, val_labels, test_data, test_labels = cv_data[0]\n",
    "\n",
    "train = utils.TensorDataset(torch.from_numpy(np.swapaxes(train_data,1,2)), torch.from_numpy(train_labels).long())\n",
    "train_loader = utils.DataLoader(train, batch_size=64, shuffle=True)\n",
    "\n",
    "valid = utils.TensorDataset(torch.from_numpy(np.swapaxes(val_data,1,2)), torch.from_numpy(val_labels).long())\n",
    "valid_loader = utils.DataLoader(valid, batch_size=64, shuffle=True)\n",
    "\n",
    "test = utils.TensorDataset(torch.from_numpy(np.swapaxes(test_data,1,2)), torch.from_numpy(test_labels).long())\n",
    "test_loader = utils.DataLoader(test, batch_size=64, shuffle=True)\n",
    "                           \n",
    "dataloaders = {'train': train_loader, 'valid': valid_loader, \"test\": test_loader}\n",
    "dataset_sizes = {'train': len(train), 'valid': len(valid), \"test\": len(test)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PJyvQdmkN_aX"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Xizp4eJJ5kYy"
   },
   "outputs": [],
   "source": [
    "def train_model(model, criteria, optimizer, scheduler, num_epochs=25, device='cuda'):\n",
    "    model.to(device)\n",
    "    since = time.time()\n",
    "    epoch_loss_array=[] \n",
    "    accuracy_array=[]\n",
    "\n",
    "    n_stages = len(class_dict.keys())\n",
    "    emission_matrices = [np.zeros((n_stages,n_stages)) for _ in range(num_epochs)]\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "    best_epoch = 0\n",
    "\n",
    "    for epoch_idx, epoch in enumerate(range(num_epochs)):\n",
    "        print('Epoch {}/{}'.format(epoch + 1, num_epochs))\n",
    "        print('-' * 10)\n",
    "\n",
    "        for phase in ['train', 'valid',\"test\"]:\n",
    "            if phase == 'train':\n",
    "                scheduler.step()\n",
    "                model.train()  \n",
    "            else:\n",
    "                model.eval()   \n",
    "                \n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "   \n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    loss = criteria(outputs, labels)\n",
    "\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "                for y_pred, y_true in zip(preds, labels):\n",
    "                    emission_matrices[epoch_idx][y_true, y_pred] += 1\n",
    "\n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            epoch_loss_array.append(epoch_loss)\n",
    "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
    "            accuracy_array.append(epoch_acc)\n",
    "\n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n",
    "                phase, epoch_loss, epoch_acc))\n",
    "\n",
    "            if phase == 'valid' and epoch_acc > best_acc:\n",
    "                best_epoch = epoch_idx\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    \n",
    "    # keep emission matrices according to the best model (biggest accuracy)\n",
    "    print(f\"Best epoch is {best_epoch}, with accuracy {best_acc:4f}\")\n",
    "    emission_matrix = emission_matrices[best_epoch]\n",
    "    emission_matrix = emission_matrix / emission_matrix.sum(axis=1, keepdims=True)\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    return model, epoch_loss_array, accuracy_array, emission_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PMXmk4v1CARM"
   },
   "outputs": [],
   "source": [
    "criteria = nn.NLLLoss() # negative log likelihood loss \n",
    "optimizer = optim.Adam(model.parameters(), lr=0.005)\n",
    "sched = optim.lr_scheduler.StepLR(optimizer, step_size=4, gamma=0.1)\n",
    "nb_epochs = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1806
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 368141,
     "status": "ok",
     "timestamp": 1546889587450,
     "user": {
      "displayName": "Tatiana Gaponova",
      "photoUrl": "https://lh5.googleusercontent.com/-gUJ79WnHrqY/AAAAAAAAAAI/AAAAAAAAQis/zrKck2fIipY/s64/photo.jpg",
      "userId": "17758316458603777674"
     },
     "user_tz": -60
    },
    "id": "Ns_SuBUhCAgP",
    "outputId": "cd0bfbcd-e4c5-49b6-cbae-644fa8493fb2"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using {}!\".format(device))\n",
    "\n",
    "model_ft,epoch_loss_array,accuracy_array,emission_matrix= train_model(model, criteria, optimizer, sched, nb_epochs, device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_array_train=[]\n",
    "loss_array_valid=[]\n",
    "loss_array_test=[]\n",
    "\n",
    "for i in range(0, len(epoch_loss_array), 3):\n",
    "    loss_array_train.append(epoch_loss_array[i])\n",
    "    \n",
    "    \n",
    "for i in range(1, len(epoch_loss_array), 3):\n",
    "    loss_array_valid.append(epoch_loss_array[i])\n",
    "\n",
    "for i in range(2, len(epoch_loss_array), 3):\n",
    "    loss_array_test.append(epoch_loss_array[i])\n",
    "plt.plot(loss_array_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(loss_array_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(loss_array_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_array_train=[]\n",
    "accuracy_array_valid=[]\n",
    "accuracy_array_test=[]\n",
    "\n",
    "for i in range(0, len(accuracy_array), 3):\n",
    "    accuracy_array_train.append(accuracy_array[i])\n",
    "    \n",
    "for i in range(1,len(accuracy_array), 3):\n",
    "    accuracy_array_valid.append(accuracy_array[i])\n",
    "\n",
    "for i in range(2, len(accuracy_array), 3):\n",
    "    accuracy_array_test.append(accuracy_array[i])\n",
    "plt.plot(accuracy_array_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(accuracy_array_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(accuracy_array_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing on our subject nights\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we want the predictions on the CPU, so that we can then display the results without busting GPU cache\n",
    "\n",
    "# model_ft = model_ft.to(\"cpu\")\n",
    "model_ft_cnn = copy.deepcopy(model_ft)\n",
    "model_ft_cnn_hmm = copy.deepcopy(model_ft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_keys = [key for key in labels if TEST_SUBJECT_FILE_PREFIX in key]\n",
    "y_test_keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (20,5)\n",
    "\n",
    "for night_key in y_test_keys:\n",
    "    y_test_current = labels[night_key]\n",
    "    curent_eeg_data = np.swapaxes(data_all[night_key], 1, 2) \n",
    "    current_eeg_tensor = torch.from_numpy(curent_eeg_data)\n",
    "    \n",
    "    output = model_ft_cnn.forward(current_eeg_tensor)\n",
    "    _, y_test_current_pred = torch.max(output, 1)\n",
    "    y_test_current_pred = y_test_current_pred.numpy()\n",
    "    \n",
    "    print(confusion_matrix(y_test_current, y_test_current_pred))\n",
    "\n",
    "    print(classification_report(y_test_current, y_test_current_pred, target_names=class_dict.values()))\n",
    "\n",
    "    print(\"Agreement score (Cohen Kappa): \", cohen_kappa_score(y_test_current, y_test_current_pred))\n",
    "    \n",
    "    print_hypnogram(\n",
    "        [y_test_current, y_test_current_pred],\n",
    "        labels=[\"scored\", \"predicted\"],\n",
    "        subject=TEST_SUBJECT_INDEX,\n",
    "        night=night_key\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hidden Markov Model\n",
    "___\n",
    "\n",
    "We add a hidden markov model as a postprecessing step to take into account the data's normal transi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_valid_keys = [key for key in labels if TEST_SUBJECT_FILE_PREFIX not in key]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title(\"Emission table probabilities\")\n",
    "plt.imshow(emission_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_components = len(class_dict.keys())\n",
    "\n",
    "def compute_hmm_matrices(y_train_valid_keys):\n",
    "    transition_matrix = np.zeros((n_components,n_components))\n",
    "    start_matrix = np.zeros((n_components))\n",
    "\n",
    "    for night_key in y_train_valid_keys:\n",
    "        print(f\"Computing file: {night_key}\")\n",
    "        current_y = labels[night_key]\n",
    "        start_matrix[current_y[0]] += 1\n",
    "\n",
    "        for transition in zip(current_y[:-1], current_y[1:]):\n",
    "            transition_matrix[transition[0], transition[1]] += 1\n",
    "            \n",
    "    transition_matrix = transition_matrix/transition_matrix.sum(axis=1, keepdims=True)\n",
    "    start_matrix = start_matrix/start_matrix.sum()\n",
    "    \n",
    "    return transition_matrix, start_matrix\n",
    "    \n",
    "transition_matrix, start_matrix = compute_hmm_matrices(y_train_valid_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title(\"Transition table probabilities\")\n",
    "plt.imshow(transition_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hmm_model = MultinomialHMM(n_components=n_components)\n",
    "\n",
    "hmm_model.transmat_ = transition_matrix\n",
    "hmm_model.startprob_ = start_matrix\n",
    "hmm_model.emissionprob_ = emission_matrix\n",
    "\n",
    "print(hmm_model.transmat_,hmm_model.startprob_, hmm_model.emissionprob_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (20,5)\n",
    "\n",
    "for night_key in y_test_keys:\n",
    "    y_test_current = labels[night_key]\n",
    "    curent_eeg_data = np.swapaxes(data_all[night_key], 1, 2) \n",
    "    current_eeg_tensor = torch.from_numpy(curent_eeg_data)\n",
    "\n",
    "    output = model_ft_cnn_hmm.forward(current_eeg_tensor)\n",
    "    _, y_test_current_pred = torch.max(output, 1)\n",
    "    y_test_current_pred = y_test_current_pred.numpy()\n",
    "    print(y_test_current_pred)\n",
    "    y_cnn_hmm_pred = hmm_model.predict(y_test_current_pred.reshape(-1, 1))\n",
    "    \n",
    "    print(confusion_matrix(y_test_current, y_cnn_hmm_pred))\n",
    "\n",
    "    print(classification_report(y_test_current, y_cnn_hmm_pred, target_names=class_dict.values()))\n",
    "\n",
    "    print(\"Agreement score (Cohen Kappa): \", cohen_kappa_score(y_test_current, y_cnn_hmm_pred))\n",
    "    \n",
    "    print_hypnogram(\n",
    "        [y_test_current, y_cnn_hmm_pred],\n",
    "        labels=[\"scored\", \"predicted\"],\n",
    "        subject=TEST_SUBJECT_INDEX,\n",
    "        night=night_key\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "sleep_stages_pytorch.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
